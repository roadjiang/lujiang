<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta content="Lu Jiang Home Page" name="Description"/>
      <meta content="Lu Jiang, lujiang, Multimedia, Computer Vision, Deep Learning, Machine Learning, Artificial Intelligence, Cloud, Generative AI, Foundation Model" name="Keywords"/>
      <link rel="shortcut icon" href="http://www.cs.cmu.edu/sites/all/themes/scs2013/favicon.ico" type="image/vnd.microsoft.icon"/>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
      <title>Lu Jiang</title>
      <link href="defaultstyles_home.css" rel="stylesheet" media="screen" />
      <script type="text/JavaScript" src="js/curvycorners.js"></script>
   </head>
   <body>
      <div id="container">
         <div id="wrapper">
            <div id="banner">
               <h1>Publications</h1>
               <p></p>
            </div>
            <div id="nav">
               <ul>
                  <li><a href="index.html">Home</a></li>
                  <li><a href="bio.html">Bio</a></li>
                  <li><a href="publications.html">Publications</a></li>
                  <li><a href="software.html">Code and Data</a></li>
                  <li><a href="CV.pdf">CV</a></li>
                  <li><a href="people.html">Interns</a></li>
                  <li><a href="contact.html">Contact</a></li>
               </ul>
            </div>
         </div>
         <div id="page">
            <div id="content">
               <div style="text-indent: 0em; padding-left: 2em;">
                  <div>
                     <hr/>
                     <h2>Full publication list: <a href="https://scholar.google.com/citations?user=jIKjjSYAAAAJ"> Google Scholar</a> or <a href="publications2.html">page</a></h2>
                     Research areas: <font class="tag-vl">Generative AI</font> - <font class="tag-learning">Robust and Trustworthy Deep Learning</font> - <font class="tag-video">Video or Multimodal Understanding</font> - <font class="tag-curriculum">Curriculum Learning</font><p/>
                     <hr/>
                     <h3><font class="tag-vl">Generative AI</font><br/></h3>
                     <hr/>
                     <ul>
                        <li>
                           <b>MAGVIT: Masked Generative Video Transformer</b><br/>
                           Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, Lu Jiang<br/>
                           CVPR 2023
                           [<a href="https://magvit.cs.cmu.edu/">project page</a>], [<a href="https://github.com/MAGVIT/magvit">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Muse: Text-To-Image Generation via Masked Generative Transformers</b><br/>
                           Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan<br/>
                           arXiv preprint arXiv:2301.00704
                           [<a href="https://arxiv.org/abs/2301.00704">pdf</a>], [<a href="https://muse-model.github.io/">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>MaskGIT: Masked Generative Image Transformer</b><br/>
                           Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman<br/>
                           CVPR 2022
                           [<a href="https://arxiv.org/abs/2202.04200">pdf</a>], [<a href="https://github.com/google-research/maskgit">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Visual Prompt Tuning for Generative Transfer Learning</b></font><br/>
                           Kihyuk Sohn, Huiwen Chang, José Lezama, Luisa Polania, , Han Zhang, Yuan Hao, Irfan Essa, Lu Jiang<br/>
                           CVPR 2023
                           [<a href="https://arxiv.org/abs/2210.00990">pdf</a>], [<a href="https://github.com/google-research/generative_transfer">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Auditing Gender Presentation Differences in Text-to-Image Models</b><br/>
                           Yanzhe Zhang, Lu Jiang, Greg Turk, Diyi Yang<br/>
                           arXiv preprint arXiv:2302.03675
                           [<a href="https://arxiv.org/abs/2302.03675">pdf</a>], [<a href="https://salt-nlp.github.io/GEP/">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>ViTGAN: Training gans with vision transformers</b> <font class="hl">[Spotlight]</font><br/>
                           Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu<br/>
                           ICLR 2022
                           [<a href="https://arxiv.org/abs/2107.04589">pdf</a>], [<a href="https://github.com/mlpc-ucsd/ViTGAN">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Discrete Predictor-Corrector Diffusion Models for Image Synthesis</b><br/>
                           Jose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, Irfan Essa<br/>
                           ICLR 2023
                           [<a href="https://openreview.net/forum?id=VM8batVBWvg">pdf</a>], [<a href="">code coming soon</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Improved Masked Image Generation with Token-Critic</b><br/>
                           Jose Lezama, Huiwen Chang, Lu Jiang, Irfan Essa<br/>
                           ECCV 2022
                           [<a href="https://arxiv.org/abs/2209.04439">pdf</a>], [<a href="https://github.com/google-research/maskgit">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Regularizing Generative Adversarial Networks under Limited Data</b><br/>
                           Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang<br/>
                           CVPR 2021
                           [<a href="https://arxiv.org/abs/2104.03310">pdf</a>], [<a href="https://github.com/google/lecam-gan">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Text as Neural Operator: Image Manipulation by Text Instruction</b><br/>
                           Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, Irfan Essa<br/>
                           ACM Multimedia 2021
                           [<a href="https://arxiv.org/abs/2008.04556">pdf</a>], [<a href="https://github.com/google/tim-gan">code</a>], [<a href="https://patents.google.com/patent/US20210383584A1/en">patent</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval</b></font><br/>
                           Hung-Yu Tseng*, Hsin-Ying Lee*, Lu Jiang, Ming-Hsuan Yang, Weilong Yang<br/>
                           ECCV 2020 (*equal contribution)
                           [<a href="https://arxiv.org/abs/2007.08513">pdf</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Neural Design Network: Graphic Layout Generation with Constraints</b> <font class="hl">[Spotlight]</font><br/>
                           Hsin-Ying Lee, Lu Jiang, Irfan Essa, Phuong B Le, Haifeng Gong, Ming-Hsuan Yang, Weilong Yang<br/>
                           ECCV 2020
                           [<a href="https://arxiv.org/abs/1912.09421">pdf</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>BLT: Bidirectional Layout Transformer for Controllable Layout Generation</b><br/>
                           Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, Irfan Essa<br/>
                           ECCV 2022
                           [<a href="https://arxiv.org/abs/2112.05112">pdf</a>], [<a href="https://github.com/google-research/google-research/tree/master/layout-blt">code</a>]
                           <br/>
                           <p/>
                        </li>
                     </ul>
                     <!-- ------------------------------------------------------------------------------------
                          ------------------------------------------------------------------------------------ -->
                     <hr/>
                     <h3><font class="tag-learning">Robust and Trustworthy Deep Learning</font><br/></h3>
                     <hr/>
                     <ul>
                        <li>
                          <b>Pyramid Adversarial Training Improves ViT Performance</b> <font class="hl">[Oral, Best paper finalist]</font><br/>
                           Charles Herrmann*, Kyle Sargent*, Lu Jiang, Ramin Zabih, Huiwen Chang, Ce Liu, Dilip Krishnan, Deqing Sun<br/>
                           CVPR 2022 (*equal contribution)
                           [<a href="https://arxiv.org/abs/2111.15121">pdf</a>], [<a href="">code coming soon</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Robust Neural Machine Translation with Doubly Adversarial Inputs</b> <font class="hl">[Best paper candidate]</font><br/>
                           Yong Cheng, Lu Jiang, Wolfgang Macherey<br/>
                           ACL 2019 
                           [<a href="https://arxiv.org/abs/1906.02443">pdf</a>], [<a href="https://ai.googleblog.com/2019/07/robust-neural-machine-translation.html">Google AI blog</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</b><br/> 
                           Lu Jiang, Di Huang, Mason Liu, Weilong Yang<br/>
                           ICML 2020
                           [<a href="https://arxiv.org/abs/1911.09781">pdf</a>], [<a href="./resources/icml20_slides.pdf">slides</a>], [<a href="https://www.youtube.com/watch?v=xcPyu_N-mEU">video</a>], [<a href="https://ai.googleblog.com/2020/08/understanding-deep-learning-on.html">Google AI blog</a>], [<a href="https://google.github.io/controlled-noisy-web-labels/index.html">dataset</a>, <a href="https://www.tensorflow.org/datasets/catalog/controlled_noisy_web_labels">tfds</a>], [<a href="https://github.com/google-research/google-research/tree/master/mentormix">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                           <b>MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</b><br/>
                           Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei<br/>
                           ICML 2018 [<a href="http://proceedings.mlr.press/v80/jiang18c/jiang18c.pdf">pdf</a>], 
                           [<a href="http://proceedings.mlr.press/v80/jiang18c/jiang18c-supp.pdf">supplementary materials</a>], 
                           [<a href="https://github.com/google/mentornet">code</a>], [<a href="./resources/mentornet_slides.pdf">slides</a>]<br/>
                           <p/>
                        </li>
                        <li>
                          <b>Confident Learning: Estimating Uncertainty in Dataset Labels</b><br/>
                           Curtis G. Northcutt, Lu Jiang, Isaac L. Chuang<br/>
                           Journal of Artificial Intelligence Research (JAIR) 2021
                           [<a href="https://arxiv.org/abs/1911.00068">pdf</a>], [<a href="https://pypi.org/project/cleanlab/">code(cleanlab)</a>].
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Regularizing Generative Adversarial Networks under Limited Data</b><br/>
                           Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang<br/>
                           CVPR 2021
                           [<a href="https://arxiv.org/abs/2104.03310">pdf</a>], [<a href="https://github.com/google/lecam-gan">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Self-supervised and Supervised Joint Training for Resource-rich Machine Translation</b><br/>
                           Yong Cheng, Wei Wang, Lu Jiang, Wolfgang Macherey<br/>
                           ICML 2021
                           [<a href="https://openreview.net/forum?id=1yDrpckYHnN">pdf</a>], [<a href="https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/mt/params/xendec">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>AdvAug: Robust Adversarial Augmentation for Neural Machine Translation</b><br/>
                           Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein <br/>
                           ACL 2020
                           [<a href="https://arxiv.org/abs/2006.11834">pdf</a>], [<a href="./resources/advaug_acl20.pdf">slides</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Faster Meta Update Strategy for Noise-Robust Deep Learning</b> <font class="hl">[Oral]</font><br/>
                           Youjiang Xu, Linchao Zhu, Lu Jiang, Yi Yang<br/>
                           CVPR 2021
                           [<a href="https://arxiv.org/abs/2104.15092">pdf</a>], [<a href="https://github.com/youjiangxu/FaMUS">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Discrete Representations Strengthen Vision Transformer Robustness</b><br/>
                           Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, Irfan Essa<br/>
                           ICLR 2022
                           [<a href="https://arxiv.org/abs/2111.10493">pdf</a>], [<a href="https://github.com/google-research/scenic">code coming soon</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>Contrastive Adaptation Network for Single- and Multi-Source Domain Adaptation</b><br/>
                           Guoliang Kang, Lu Jiang, Yunchao Wei, Yi Yang, Alexander G. Hauptmann<br/>
                           IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2021
                           [<a href="https://ieeexplore.ieee.org/document/9219132">pdf</a>], [<a href="https://github.com/kgl-prml/Contrastive-Adaptation-Network-for-Unsupervised-Domain-Adaptation">code</a>].
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>SimAug: Learning Robust Representations from Simulation for Trajectory Prediction</b></font><br/>
                           Junwei Liang, Lu Jiang, Alexander Hauptmann<br/>
                           ECCV 2020
                           [<a href="https://arxiv.org/abs/2004.02022">pdf</a>], [<a href="https://next.cs.cmu.edu/simaug/">code</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                           <b>Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data</b><br/>
                           Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann<br/>
                           ICMR 2017 [<a href="./camera_ready_papers/icmr17.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                     </ul>
                     <!-- ------------------------------------------------------------------------------------
                          ------------------------------------------------------------------------------------ -->
                     <hr/>
                     <h3><font class="tag-video">Video or Multimodal Understanding</font><br/></h3>
                     <hr/>
                     <ul>
                        <li>
                          <b>Peeking into the future: Predicting Future Person Activities and Locations in Videos</b><br/>
                           Junwei Liang, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei<br/>
                           CVPR 2019 
                           [<a href="https://arxiv.org/abs/1902.03748">pdf</a>], [<a href="https://next.cs.cmu.edu/">code</a>], [<a href="https://www.youtube.com/watch?v=NyrGxGoS01U">demo video</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                          <b>The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction</b><br/>
                           Junwei Liang, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann <br/>
                           CVPR 2020
                           [<a href="https://arxiv.org/abs/1912.06445">pdf</a>], [<a href="https://next.cs.cmu.edu/multiverse/">code</a>], [<a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">blog post</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                           <b>Composing Text and Image for Image Retrieval - An Empirical Odyssey</b> <font class="hl">[Oral]</font><br/>
                           Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays<br/>
                           CVPR 2019
                           [<a href="https://arxiv.org/abs/1812.07119">pdf</a>], [<a href="https://github.com/google/tirg">code</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Eidetic 3D LSTM: A Model for Video Prediction and Beyond</b><br/>
                           Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, Li Fei-Fei<br/>
                           ICLR 2019 
                           [<a href="https://openreview.net/forum?id=B1lKS2AqtX">pdf</a>], [<a href="https://github.com/google/e3d_lstm">code</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Focal Visual-Text Attention for Memex Question Answering.</b><br/>
                           Junwei Liang, Lu Jiang, Liangliang Cao, Yannis Kalantidis, Li-Jia Li, Alexander Hauptmann<br/>
                           IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2019 [<a href="">pdf</a>], [<a href="https://memexqa.cs.cmu.edu">dataset</a>],
                           [<a href="https://github.com/JunweiLiang/MemexQA_StarterCode">code</a>], [<a href="https://www.youtube.com/watch?v=ieh6Eh9_Dc0">demo video</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Focal Visual-Text Attention for Visual Question Answering</b><font class="hl"> [Spotlight]</font><br/>
                           Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, Alexander Hauptmann<br/>
                           CVPR 2018 [<a href="https://ai.google/research/pubs/pub47012">pdf</a>], 
                           [<a href="https://memexqa.cs.cmu.edu/fvta.html">project page</a>]<br/>
                           <p/>
                        </li>

                        <li>
                          <b>Revisiting EmbodiedQA: A Simple Baseline and Beyond</b><br/>
                           Yu Wu, Lu Jiang, Yi Yang<br/>
                           IEEE Transactions on Image Processing 2020 
                           [<a href="https://arxiv.org/pdf/1904.04166.pdf">pdf</a>]
                           <br/>
                           <p/>
                        </li>
                        <li>
                           <b>Graph Distillation for Action Detection with Privileged Information</b><br/>
                           Zelun Luo, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li Fei-Fei<br/>
                           ECCV 2018 [<a href="https://arxiv.org/abs/1712.00108">pdf</a>],
                           [<a href="https://github.com/google/graph_distillation">project page</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Switchable Novel Object Captioner</b><br/>
                           Yu Wu, Lu Jiang, Yi Yang<br/>
                           TPAMI 2022 [<a href="https://ieeexplore.ieee.org/abstract/document/9693277">pdf</a>] and ACM MM 2018 [<a href="https://arxiv.org/abs/1804.03803">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Delving Deep into Personal Photo and Video Search</b><br/>
                           Lu Jiang, Yannis Kalantidis, Liangliang Cao, Sachin, Farfade, Jiliang Tang, Alex Hauptmann<br/>
                           WSDM 2017 [<a href="./camera_ready_papers/wsdm17.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Revealing Event Saliency in Unconstrained Video Collection</b><br/>
                           Dingwen Zhang, Junwei Han, Lu Jiang, Senmao Ye, Xiaojun Chang<br/>
                           IEEE Transactions on Image Processing 26(4): 1746-1758 2017 [<a href="">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Web-scale Multimedia Search for Internet Video Content</b><br/>
                           Lu Jiang<br/>
                           WWW 2016 [<a href="./camera_ready_papers/WWW16.pdf">pdf</a>], [<a href="./resources/Thesis.pdf">full thesis</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Fast and Accurate Content-based Semantic Search in 100M Internet Videos</b><br/>
                           Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mitamura, Alexander Hauptmann<br/>
                           ACM MM 2015, [<a href="./camera_ready_papers/MM15.pdf">pdf</a>], [<a href="./resources/search100m.pdf">slides</a>], 
                           [<a href="https://sites.google.com/site/videosearch100m/">project page</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Bridging the Ultimate Semantic Gap: A Semantic Search Engine for Internet Videos</b><font class="hl"> [Best paper candidate]</font><br/>
                           Lu Jiang, Shoou-I Yu, Deyu Meng, Teruko Mitamura, Alexander Hauptmann<br/>
                           ICMR 2015, [<a href="./camera_ready_papers/ICMR2015_SemanticEngine.pdf">pdf</a>],
                           [<a href="./camera_ready_papers/ICMR2015_SemanticEngine_supplementary_materials.pdf">slides</a>],
                           [<a href="./0Ex/icmr15.html">project page</a>]<br/>
                        </li>
                        <li>
                           <b>Content-Based Video Search over 1 Million Videos with 1 Core in 1 Second</b><br/>
                           Shoou-I Yu, Lu Jiang, Zhongwen Xu, Yi Yang and Alexander Hauptmann<br/>
                           ICMR 2015, [<a href="./camera_ready_papers/ICMR2015_1M.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Zero-Example Event Search using MultiModal Pseudo Relevance Feedback</b><br/>
                           Lu Jiang, Teruko Mitamura, Shoou-I Yu, Alexander Hauptmann<br/>
                           ICMR 2014, [<a href="./camera_ready_papers/ICMR2014-MMPRF.pdf">pdf</a>], 
                           [<a href="./resources/TRECVID_MED13_NIST.pdf">slides</a>]<br/>
                           <p/>
                        </li>

                        <li>
                           <b>Leveraging High-level and Low-level Features for Multimedia Event Detection</b><br/>
                           Lu Jiang, Alexander Hauptmann, Guang Xiang<br/>
                           ACM MM 2012, [<a href="./camera_ready_papers/ACM_MM_2012.pdf">pdf</a>], 
                           [<a href="./resources/Leveraging%20High-level%20and%20Low-level%20Features%20for%20Multimedia%20Event%20Detection.2nd.revised.pdf">slides</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>CMU-Informedia@ TRECVID 2014</b><font class="hl"> [Top performer in the TRECVID multimedia event detection task 2014]</font><br/>
                           Shoou-I Yu, Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi Mao, Chuang Gan, Yajie Miao, Xingzhong Du,Yang Cai, Lara Martin, Nikolas Wolfe, Anurag Kumar,
                           Huan Li, Ming Lin, Zhigang Ma, Yi Yang, Deyu Meng, Shiguang Shan, Pinar Duygulu Sahin, Susanne Burger, Florian Metze, Rita Singh, Bhiksha Raj, Teruko Mitamura, Richard Stern and Alexander Hauptmann.<br/>
                           NIST TRECVID 2014 [<a href="./camera_ready_papers/informedia_MED14.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>CMU-Informedia@ TRECVID 2013</b><font class="hl"> [Top performer in the TRECVID multimedia event search task 2013]</font><br/>
                           Zhen-Zhong Lan, Lu Jiang, Shoou-I Yu, Shourabh Rawat, Yang Cai, Chenqiang Gao, Shicheng Xu, Haoquan Shen, Xuanchong Li, Yipei Wang, Waito Sze, Yan Yan, Zhigang Ma, Nicolas Ballas, Deyu Meng,
                           Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, Bhiksha Raj, Richard Stern, Teruko Mitamura, Eric Nyberg, and Alexander Hauptmann<br/>
                           NIST TRECVID 2013 [<a href="./camera_ready_papers/informedia@trecvid2013.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Towards Efficient Learning of Optimal Spatial Bag-of-Words Representations</b> <font class="hl"> [Best paper candidate]</font><br/>
                           Lu Jiang, Wei Tong, Deyu Meng, Alexander Hauptmann<br/>
                           ICMR 2014, [<a href="./camera_ready_papers/ICMR2014-Towards.pdf">pdf</a>], 
                           [<a href="./resources/JSTiling_Presentation.pdf">slides</a>]<br/>
                           <p/>
                        </li>
                     </ul>
                     <!-- ------------------------------------------------------------------------------------
                          ------------------------------------------------------------------------------------ -->
                     <hr/>
                     <h3><font class="tag-curriculum">Curriculum Learning</font><br/></h3>
                     <hr/>
                     <ul>
                        <li>
                           <b>MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</b><br/>
                           Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei<br/>
                           ICML 2018 [<a href="http://proceedings.mlr.press/v80/jiang18c/jiang18c.pdf">pdf</a>], 
                           [<a href="http://proceedings.mlr.press/v80/jiang18c/jiang18c-supp.pdf">supplementary materials</a>], 
                           [<a href="https://github.com/google/mentornet">code</a>], [<a href="./resources/mentornet_slides.pdf">slides</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Self-paced Curriculum Learning.</b> <font class="hl">[Oral]</font><br/>
                           Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, Alexander Hauptmann<br/>
                           AAAI 2015, [<a href="./camera_ready_papers/AAAI_SPCL_2015.pdf">pdf</a>], 
                           [<a href="./camera_ready_papers/AAAI_SPCL_2015_supplementary_materials.pdf">supplementary materials</a>],
                           [<a href="./applications/spcl_toy_examples.zip">demo code</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Self-paced Learning with Diversity</b><br/>
                           Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, Alexander Hauptmann<br/>
                           NIPS 2014, [<a href="./camera_ready_papers/NIPS_2014.pdf">pdf</a>], 
                           [<a href="./camera_ready_papers/NIPS_2014_supplementary_materials.pdf">supplementary materials</a>], 
                           [<a href="./spld">project page</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search</b><br/>
                           Lu Jiang, Deyu Meng, Teruko Mitamura, Alexander Hauptmann<br/>
                           ACM MM 2014, [<a href="./camera_ready_papers/ACM_MM_fp_2014.pdf">pdf</a>],
                           [<a href="./resources/SPaR1105.pdf">slides</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Self-paced Learning for Matrix Factorization</b><br/>
                           Qian Zhao, Deyu Meng, Lu Jiang, Qi Xie, Zongben Xu, Alexander Hauptmann<br/>
                           AAAI 2015, [<a href="./camera_ready_papers/AAAI_MF_2015.pdf">pdf</a>], 
                           [<a href="./camera_ready_papers/AAAI_MF_2015_supplementary_materials.pdf">supplementary materials</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Learning to Detect Concepts from Webly-Labeled Video Data.</b><br/>
                           Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann<br/>
                           IJCAI 2016, [<a href="./camera_ready_papers/IJCAI_2016.pdf">pdf</a>], [<a href="https://www.youtube.com/watch?v=3he6VDwYMCQ">demo video</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>A Self-Paced Multiple-Instance Learning Framework for Co-Saliency Detection</b><br/>
                           Dingwen Zhang, Deyu Meng, Chao Li, Lu Jiang, Qian Zhao, Junwei Han<br/>
                           ICCV 2015, [<a href="">pdf</a>]<br/>
                           <p/>
                        </li>
                     </ul>
                     <!-- ------------------------------------------------------------------------------------
                          ------------------------------------------------------------------------------------ -->
                     <hr/>
                     <h3>Miscellaneous<br/></h3>
                     <hr/>
                     <ul>
                        <li>
                           <b>Improvements to Speaker Adaptive Training of Deep Neural Networks</b><font class="hl"> [Best poster at SLT]</font><br/>
                           Yajie Miao, Lu Jiang, Hao Zhang, Florian Metze<br/>
                           SLT 2014, [<a href="./camera_ready_papers/SLT2014.pdf">pdf</a>], 
                           [<a href="http://www.cs.cmu.edu/~ymiao/satdnn.html">project page</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Viral Video Style: A Closer Look at Viral Videos on YouTube</b><font class="hl"> [Best poster at CMU-LTI SRS14]</font><br/>
                           Lu Jiang, Yajie Miao, Yi Yang, Zhen-Zhong Lan, Alexander Hauptmann<br/>
                           ICMR 2014, [<a href="./camera_ready_papers/ICMR2014-Viral.pdf">pdf</a>], 
                           [<a href="./resources/ViralVideos.pdf">slides</a>], [<a href="https://sites.google.com/site/cmuviralvideos/characteristics">dataset</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Mining Learning-Dependency between Knowledge Units from Text</b><br/>
                           Jun Liu, Lu Jiang, Zhaohui Wu, Qinghua Zheng, Yanan Qian<br/>
                           The VLDB Journal, 20(3): 335-345, 2011, [<a href="./camera_ready_papers/VLDBJ.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>A Peep at Pornography Web in China</b><br/>
                           Zhaohui Wu, Lu Jiang, Qinghua Zheng, Zhenhua Tian, Jun Liu, Junzhou Zhao<br/>
                           Web of Science, 2010 [<a href="./camera_ready_papers/WEBSCI_2010.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                        <li>
                           <b>Efficient Deep Crawling using Reinforcement Learning</b><br/>
                           Lu Jiang, Zhaohui Wu, Qian Feng, Jun Liu, Qinghua Zheng<br/>
                           PAKDD, 2010 [<a href="./camera_ready_papers/PAKDD_2010.pdf">pdf</a>]<br/>
                           <p/>
                        </li>
                     </ul>
                  </div>
               </div>
            </div>
         </div>
         <div id="footer">&copy; 2017 Lu Jiang All Rights Reserved</div>
      </div>
   </body>
</html>
